<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on Liupu&#39;s Blog</title>
    <link>https://liupu14.github.io/tags/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on Liupu&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 16 Mar 2019 09:57:18 +0000</lastBuildDate>
    
	<atom:link href="https://liupu14.github.io/tags/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>你租贵了吗？——R语言与链家租房数据分析</title>
      <link>https://liupu14.github.io/posts/%E4%BD%A0%E7%A7%9F%E8%B4%B5%E4%BA%86%E5%90%97r%E8%AF%AD%E8%A8%80%E4%B8%8E%E9%93%BE%E5%AE%B6%E7%A7%9F%E6%88%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</link>
      <pubDate>Sat, 16 Mar 2019 09:57:18 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/%E4%BD%A0%E7%A7%9F%E8%B4%B5%E4%BA%86%E5%90%97r%E8%AF%AD%E8%A8%80%E4%B8%8E%E9%93%BE%E5%AE%B6%E7%A7%9F%E6%88%BF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2019-01-30&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;开年后的广州很疯狂呀，买房人群又一次暴涨，难道是下一次高潮又来了吗（想想而已，短期内房价的飞涨已经不可能了）？不过虽然飞涨不可能，但是伴随着这么高的买房人群房价总是需要上涨一番的，这就引起了我等租房人群的喜悦。想想买房人多了，租房人就相对减少了，那么根据供需法则，房租是不是应该降了呀？本着好奇的心情，小编就想大致了解一下目前租房市场的大致房租水平，当然了目前也属于租房高峰期，所以借此文也希望能够给想要租房的朋友一个直观的认识，让你明白你在租房的过程中是否真的租到了价位比较合适的房源了。关于租房的信息，目前存在着多种平台，但是小编个人还是比较喜欢链家的，因为其它几个平台真的存在着很多虚价房（这里绝无抹黑其它平台的意思，只是在强调一个事实而已），所以小编准备以链家网上的租房信息来说明目前广州市的房租态势。考虑到花都、从化、增城以及南沙这些郊区中的郊区租房人群较少（去那边玩小编会很开心，如果让小编去那边住，小编只能呵呵了），所以小编将紧紧以其它七个区域（天河区、越秀区、荔湾区、海珠区、白云区、番禺区、黄浦区）的租房数据进行说明，下面正式进入！&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>网页解析工具终结篇——xpath语法</title>
      <link>https://liupu14.github.io/posts/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7%E7%BB%88%E7%BB%93%E7%AF%87xpath%E8%AF%AD%E6%B3%95/</link>
      <pubDate>Mon, 17 Sep 2018 21:45:31 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7%E7%BB%88%E7%BB%93%E7%AF%87xpath%E8%AF%AD%E6%B3%95/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-9-14&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;山竹（难怪都说越是美丽的东西越危险，这么美的名字下竟是如此狂野）终于过去了，囤货泡面的日子真的不好受，希望天佑中华，无狂风再至！本期文章接着和大家讲一下相关网页解析工具xpath。如果大家熟悉文档路径结构的话，那么xpath绝对是一种较为容易掌握的网页解析方式，这也是小编为什麼将其放在最后一期网页解析系列文章的原因之一。小编一直觉得学东西一定要先啃掉难的，而后再去掌握简单的（或者说此时你只需要较少的时间投入便可掌握简单的知识）。废话就不多说了，下面正式开始本期文章的介绍。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>送上一碗鸡汤——BeautifulSoup网页解析介绍</title>
      <link>https://liupu14.github.io/posts/%E9%80%81%E4%B8%8A%E4%B8%80%E7%A2%97%E9%B8%A1%E6%B1%A4beautifulsoup%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Fri, 14 Sep 2018 23:12:30 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/%E9%80%81%E4%B8%8A%E4%B8%80%E7%A2%97%E9%B8%A1%E6%B1%A4beautifulsoup%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90%E4%BB%8B%E7%BB%8D/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-9-10&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;上期讲述了正则表达式在网页解析中的用法，了解了怎么使用正则表达式去获取网页数据。然而正则表达式语法相对较难掌握以及正则表达式书写相对繁琐，所以就相继有了其它一些网页解析方式。本期就和大家讲一下其中的代表BeautifulSoup，可以说这种解析工具在网页解析中是使用最为方便的一种解析方式，其清晰便捷的语法规则真的如其名字所彰显的那样（美丽心灵一般不会作恶），下面正式开始对其的讲解。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>爬虫风雨路，正则知多少？</title>
      <link>https://liupu14.github.io/posts/%E7%88%AC%E8%99%AB%E9%A3%8E%E9%9B%A8%E8%B7%AF%E6%AD%A3%E5%88%99%E7%9F%A5%E5%A4%9A%E5%B0%91/</link>
      <pubDate>Mon, 10 Sep 2018 22:43:46 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/%E7%88%AC%E8%99%AB%E9%A3%8E%E9%9B%A8%E8%B7%AF%E6%AD%A3%E5%88%99%E7%9F%A5%E5%A4%9A%E5%B0%91/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-9-7&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;原计划接下来的几期文章聊一下pandas库以及机器学习的那些事，奈何终究还是受不了一些朋友的反馈，希望小编讲一些网页解析的知识。既然如此，小编就满足一下这些朋友的要求，接下来几期就和大家聊一下网络爬虫中网页解析方面的相关知识。这将主要围绕正则表达式、lxml、beautifulsoup以及pyquery四种解析工具展开。因为正则表达式是所有解析方式中最高效以及适用性最高的工具，所以这期文章就先和大家讲一下正则表达式的相关用法，下面正式开始。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>温故知新——python爬虫阶段总结</title>
      <link>https://liupu14.github.io/posts/%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0python%E7%88%AC%E8%99%AB%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%93/</link>
      <pubDate>Mon, 03 Sep 2018 22:02:52 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/%E6%B8%A9%E6%95%85%E7%9F%A5%E6%96%B0python%E7%88%AC%E8%99%AB%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%93/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-8-31&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;前几期文章中，小编先后和大家讲述了四篇关于python爬取的文章，分别涉及到的主题是分页面以及跨页面网页数据的爬取、爬取数据怎么存储的Excel以及相关的数据库系统之中，通过四期文章的介绍，大致可以了了python爬虫的基本原理及步骤。所谓温故而知新，因此本期小编准备对那几期文章及python爬虫做一个总结，从而使大家更加深刻地认识到python爬虫。闲话不多说，下面正式进入主题。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>爬虫数据的数据库存储——MySQL数据库</title>
      <link>https://liupu14.github.io/posts/%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8mysql%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Fri, 31 Aug 2018 21:16:50 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8mysql%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-8-28&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;上期文章，小编和大家聊了一下怎么使用MongoDB数据库去存储python爬取到的数据，本期小编准备再接再厉，接着介绍一下关系型数据库和爬取数据的存储。关系型数据库一直以来都是市场的主流，因此存在众多的关系型数据库管理系统，然而在这众多的数据库管理系统中，小编尤为推荐MySQL这款开源数据库，因此本文将讲解怎么将爬取到的数据存储到MySQL数据库之中，这期爬取的内容小编以豆瓣上面排名前250的书籍信息获取为主，下面正式开始。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>爬虫数据的数据库存储——MongoDB数据库</title>
      <link>https://liupu14.github.io/posts/%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8mongodb%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Mon, 27 Aug 2018 21:24:12 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%98%E5%82%A8mongodb%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-8-24&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;关于python网络爬虫，小编在前面已经讲解了两期，分别讲述了多页面爬虫和跨页面爬虫的实现方式。同时在那两期文章中，小编都是采用Excel作为存储数据的方式，然而当面临较大的数据存储需求时，Excel相对来说缺乏效率，因此本期文章，小编将介绍怎么将python爬取下来的数据存放在数据库之中。从现有使用程度来讲，数据库主要分为关系型数据库以及非关系型数据库，所以小编会分两期对这两种数据库的存储予以介绍。在关系型数据库方面，小编将重点介绍MySQL；而在非关系型数据库方面，小编将以MongoDB为主。本期文章先来介绍一下怎么将python爬取下来的数据存储到MongoDB数据库之中。同时本期文章以爬取豆瓣电影中排名前250的影片信息为例进行讲解，下面正式开始。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>python爬取链家网租房数据</title>
      <link>https://liupu14.github.io/posts/python%E7%88%AC%E5%8F%96%E9%93%BE%E5%AE%B6%E7%BD%91%E7%A7%9F%E6%88%BF%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Mon, 20 Aug 2018 21:15:46 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/python%E7%88%AC%E5%8F%96%E9%93%BE%E5%AE%B6%E7%BD%91%E7%A7%9F%E6%88%BF%E6%95%B0%E6%8D%AE/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-8-17&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;前期文章和大家聊了一下怎么爬取多页面网页，了解了爬虫的基本原理及步骤，那么本期小编准备再接再厉，介绍一下跨页面爬虫原理。考虑到八月将逝，九月即来，新的租房需求即将诞生，本期小编就来讲一下怎么使用python爬取链家网上面的租房信息，希望对大家的租房有所帮助。下面正式开始！&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>python爬取酷狗Top500排行榜</title>
      <link>https://liupu14.github.io/posts/python%E7%88%AC%E5%8F%96%E9%85%B7%E7%8B%97top500%E6%8E%92%E8%A1%8C%E6%A6%9C/</link>
      <pubDate>Mon, 13 Aug 2018 21:45:22 +0000</pubDate>
      
      <guid>https://liupu14.github.io/posts/python%E7%88%AC%E5%8F%96%E9%85%B7%E7%8B%97top500%E6%8E%92%E8%A1%8C%E6%A6%9C/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;微信公众号：Python商务实践
博客网址：&lt;a href=&#34;www.liupu.top&#34;&gt;&lt;a href=&#34;http://www.liupu.top&#34;&gt;www.liupu.top&lt;/a&gt;&lt;/a&gt;
任何问题和建议，请在博客评论区或公众号留言
最近更新时间：&lt;code&gt;2018-8-9&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;heading&#34;&gt;写在前面&lt;/h3&gt;
&lt;p&gt;新的一周开始了，今天无意间听到一首比较好听的歌曲“学猫叫”，然后就开始了无限循环模式。本以为是新出的歌曲，故怀着激动的心情推荐给朋友，谁知朋友直接告诉我这首歌已经在酷狗top500排行榜高居榜首很久了。听到这个，不得不感慨，我是有多久没有了解酷狗的歌曲信息了。因此怀着好奇的心情去看了一下最新的酷狗top500排行，发现这首歌曲仍高居榜首，另外排行榜中也出现了很多我没有听说过的歌曲，真的打击到我了。对我这个喜欢听音乐的人来说，竟然对当下比较流行的一些歌曲竟然缺乏了基本认识。在这种耻辱心理的作用下，小编就想着统计一下最近的酷狗top500排行榜的歌曲信息，因此本期文章小编就和大家分享一下怎么使用python去爬取酷狗top500排行榜的歌曲信息，娱乐之余随时掌握点知识。下面正式开始。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>